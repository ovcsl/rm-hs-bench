# rm-hs-bench: Supplementary Material

This repository contains supplementary data, evaluation metrics, and reproducibility scripts for the paper **"Applying Neuro-Symbolic AI with Rulemapping"**.

## ðŸ“‚ Repository Contents

This repository contains system-generated outputs and analysis tools to reproduce the results reported in the paper.

## ðŸ“š Original Dataset

This work builds upon the **Legal Hate Speech Dataset**. The raw social media posts used in our analysis were sourced from this dataset.

*   **Citation:** [A Legal Approach to Hate Speech â€“ Operationalizing the EUâ€™s Legal Framework against the Expression of Hatred as an NLP Task](https://aclanthology.org/2022.nllp-1.5/) (Zufall et al., NLLP 2022)
*   **Access:** Researchers wishing to analyze the raw text content must obtain the original dataset directly from the authors. Our repository provides only `Post_IDs` to facilitate mapping back to this source. A part of the dataset is freely available here: [https://github.com/simulacrum6/op-hate-nlp](https://github.com/simulacrum6/op-hate-nlp)

### 1. Classification Data (`cleaned_results_*.csv`)
These files contain the binary classification labels (punishable vs. not punishable) generated by the system for each post.
*   **`cleaned_results_hatespeech_expert.csv`**: System predictions compared against the **expert** annotator ground truth.
*   **`cleaned_results_hatespeech_lay.csv`**: System predictions compared against the **lay** annotator consensus.

> **Note:** In compliance with GDPR and data privacy regulations, the **raw text** of the social media posts is excluded. These files use Post IDs to map to the original *Legal Hate Speech Dataset*.

### 2. Evaluation Metrics (`evaluation_results_*.csv`)
These files contain the pre-computed performance summaries (Precision, Recall, F1-scores, Cohen's Kappa) for all tested models.
*   **`evaluation_results_expert.csv`**: Metrics derived from the expert dataset.
*   **`evaluation_results_lay.csv`**: Metrics derived from the lay consensus dataset.

### 3. Reproducibility (`Evaluation.py`)
*   **`Evaluation.py`**: A Python script used to calculate the performance metrics from the result files. It computes Accuracy, F1, F2, Balanced Accuracy, and Cohen's Kappa.

## ðŸš€ Usage

To reproduce the metrics, you can run the provided Python script on the result CSV files. The script evaluates all model columns located **to the right** of the specified gold-standard column.

### Prerequisites
*   Python 3.8+
*   Required libraries: `pandas`, `numpy` (install via `pip install pandas numpy`)

### Running the Evaluation
The basic command requires the input file and the name of the ground-truth column.

```bash
# Example: Evaluate lay annotator results
python Evaluation.py "cleaned_results_hatespeech_lay.csv" --gold "Â§130 StGB - Goldstandard" --output "my_reproduction_results.csv"


### Options
--gold: (Required) The exact name of the column containing the ground truth (e.g., "Â§130 StGB - Goldstandard").

--sep: CSV delimiter (default: ,). Use ; if your CSV uses semicolons.

--encoding: File encoding (default: utf-8). Try latin1 if you see encoding errors.

--include-empty-as-negative: By default, empty cells in prediction columns are ignored. Use this flag to treat them as "Not Punishable" (0/N).

### License
This supplementary material is made available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.
